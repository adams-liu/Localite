{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localite Kingston Utilities Project\n",
    "\n",
    "This notebook gives the preliminary steps in developing asset failures on an eletrical grid by leveraging machine learning.\n",
    "<br> We partnered with Kingston Utilites to first assists them in identifying high-risk customer meters that can potentially fail in the future.\n",
    "<br> For any further technical questions please contact Adams Liu at adamsliu97@gmail.com. \n",
    "<br> Thank you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step 1 - Data Merging\n",
    "Issue: The first 2016 service logs data is very large, each month consists of 0.5GB to a total of 12GB for one year.\n",
    "       <br> Since we do not have a server I downloaded all the log files seperately and I created a new csv files that capture the total amount of KWH per month (eg. <i> KWH_Report_Jan.csv </i>)\n",
    " <br> This would make it fasters to process relevant data in the future.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute difference between to dates\n",
    "\n",
    "def days_between(aYr, aMon, aDay, bYr, bMon, bDay):\n",
    "    d0 = date(aYr, aMon, aDay)\n",
    "    d1 = date(bYr, bMon, bDay)\n",
    "    delta = d1 - d0\n",
    "    return delta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of all .csv files\n",
    "lsReports = [\"Electric-2016-Apr.csv\",\"electric-2016-Aug.csv\",\"Electric-2016-Dec.csv\",\"electric-2016-Feb.csv\",\"electric-2016-Jan.csv\",\"Electric-2016-Jul.csv\",\"Electric-2016-Jun.csv\",\n",
    "             \"electric-2016-Mar.csv\",\"Electric-2016-May.csv\",\"Electric-2016-Nov.csv\",\"electric-2016-Oct.csv\",\"electric-2016-Sep.csv\"]\n",
    "lsMonthes = [\"Apr\",\"Aug\",\"Dec\",\"Feb\",\"Jan\",\"Jul\",\"Jun\",\"Mar\",\"May\",\"Nov\",\"Oct\",\"Sep\"]\n",
    "\n",
    "#We will need to create DataFrames to be able to find so interesting datapoints \n",
    "lsMax = pd.DataFrame()\n",
    "lsZeros= pd.DataFrame()\n",
    "lsLess = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the sum of all the monthes with unique ServiceIDs and covert them to .csv files\n",
    "i = 0\n",
    "for files in lsReports:\n",
    "    df = pd.read_csv(files)\n",
    "    df = df.sort_values(by='KWH',ascending=False)\n",
    "    lsMax = lsMax.append(df.head(50))\n",
    "    lsLess = lsLess.append(df[df.KWH < 0])\n",
    "    lsZeros = lsZeros.append(df[df.KWH == 0])\n",
    "    df2 = df[[\"SERVICEID\",\"KWH\"]].groupby([\"SERVICEID\"]).sum().to_csv(\"KWH_Report_\" +lsMonthes[i]+\".csv\")\n",
    "    i+=1 \n",
    "\n",
    "#Have a seperate .csv for KWH_Max(top 50 KWH/month), KWH_LESS(all negative KWH), KWH_Zeros(all KWH that were zeros)\n",
    "lsMax.to_csv(\"KWH_Max.csv\")\n",
    "lsZeros.to_csv(\"KWH_Zeros.csv\")\n",
    "lsLess.to_csv(\"KWH_Less.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging meters to get postal code files \n",
    "meters = pd.read_csv(\"meters.csv\")\n",
    "spCode = pd.read_csv(\"ServicePostalCode.csv\") #there are less serviceIDs in the ServicePostalCode\n",
    "\n",
    "meters.sort_values(by=['USDP_ID'])\n",
    "spCode.sort_values(by=['service ID'])\n",
    "metersMerged = pd.merge(meters, spCode, left_on = 'USDP_ID', right_on = 'service ID', how = 'left')\n",
    "\n",
    "\n",
    "## Created the YYYY-MM-DD-HH attributes for the dataframe for both the start-date and end-date\n",
    "metersMerged['START_YEAR'] = pd.DatetimeIndex(metersMerged['EFFECTIVE_DATE']).year\n",
    "metersMerged['START_MONTH'] = pd.DatetimeIndex(metersMerged['EFFECTIVE_DATE']).month\n",
    "metersMerged['START_DAY'] = pd.DatetimeIndex(metersMerged['EFFECTIVE_DATE']).day\n",
    "metersMerged['START_HOUR'] = pd.DatetimeIndex(metersMerged['EFFECTIVE_DATE']).hour\n",
    "metersMerged['END_YEAR'] = pd.DatetimeIndex(metersMerged['END_DATE']).year\n",
    "metersMerged['END_MONTH'] = pd.DatetimeIndex(metersMerged['END_DATE']).month\n",
    "metersMerged['END_DAY'] = pd.DatetimeIndex(metersMerged['END_DATE']).day\n",
    "metersMerged['END_HOUR'] = pd.DatetimeIndex(metersMerged['END_DATE']).hour\n",
    "\n",
    "## Created number of days between effective date and end data column\n",
    "daysDiff = (pd.to_datetime(metersMerged['END_DATE']) - pd.to_datetime(metersMerged['EFFECTIVE_DATE'])).dt.days\n",
    "metersMerged['DAYS_DIFF'] = daysDiff\n",
    "\n",
    "\n",
    "## Reordered the columns\n",
    "new_columns = [\"USDP_ID\",\"EQUIPMENT_NUMBER\",\"EFFECTIVE_DATE\",\n",
    "               \"START_YEAR\",\"START_MONTH\",\"START_DAY\",\"START_HOUR\",\n",
    "               \"END_DATE\",\"END_YEAR\",\"END_MONTH\",\"END_DAY\",\"END_HOUR\",\n",
    "              \"DAYS_DIFF\",\"postal code\"]\n",
    "\n",
    "## Renamed some columns\n",
    "metersMerged = metersMerged.reindex(columns=new_columns)\n",
    "metersMerged.columns.values[0] = \"SERVICEID\" \n",
    "metersMerged.columns.values[12] = \"DAYS_DIFF\" \n",
    "metersMerged.columns.values[13] = \"POST_CODE\" \n",
    "\n",
    "\n",
    "## Make a space in between the POST_CODE\n",
    "for i in range(metersMerged[\"POST_CODE\"].values.size):\n",
    "    if(isinstance(metersMerged[\"POST_CODE\"].values[i] , str)):\n",
    "        metersMerged[\"POST_CODE\"].values[i] = metersMerged[\"POST_CODE\"].values[i][:3]+\" \" + metersMerged[\"POST_CODE\"].values[i][-3:]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "## Covert to csv file\n",
    "metersMerged.to_csv(\"metersMerged.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Originally wanted to get the LATs and LONs of each individual customer but we drew too much traffic from the server \n",
    "## (may need to get the LATs and LONs manually)\n",
    "\n",
    "# #Installing geopy API\n",
    "# !pip install geopy\n",
    "# import geopy as gp\n",
    "# from geopy.geocoders import Nominatim\n",
    "# import pgeocode as pgc\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "# nomi = pgc.Nominatim('CA')\n",
    "# lat = []\n",
    "# lon = []\n",
    "# for i in range(metersNew[\"POSTCODE\"].values.size):\n",
    "#     if(isinstance(metersNew[\"POSTCODE\"].values[i] , str)):\n",
    "#         location = geolocator.geocode(metersNew[\"POSTCODE\"][i])\n",
    "        \n",
    "#         if(location is None):\n",
    "#             lat.append(nomi.query_postal_code(location).latitude)\n",
    "#             lon.append(nomi.query_postal_code(location).longitude)\n",
    "\n",
    "#         else:\n",
    "#             lat.append(location.latitude)\n",
    "#             lon.append(location.longitude)   \n",
    "#     else:\n",
    "#         pass\n",
    "# print(i)\n",
    "# metersNew[\"LAT\"] = lat\n",
    "# metersNew[\"LON\"] = lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Step 2 - Finding True Labels\n",
    "Issue: With multiple datasources the goal was to isolate those datapoints that have true labels. This was done in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the new mergedMeters.csv data (36095 rows x 14 cols) it will now be called \"dfmetersNew\"\n",
    "dfmetersNew = pd.read_csv(\"metersMerged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Find all the serviceIDs that have start year and end year as 2016\n",
    "all2016 = []\n",
    "for index, row in dfmetersNew.iterrows():\n",
    "    if(row[\"END_YEAR\"]==2016 and row[\"START_YEAR\"]==2016):\n",
    "        all2016.append(row)\n",
    "        \n",
    "\n",
    "# (2) Filter out for only instances where there is a postal codes\n",
    "\n",
    "df2016 = pd.DataFrame(all2016)\n",
    "postCodeBool = pd.notnull(df2016[\"POST_CODE\"]) \n",
    "df2016new = df2016[postCodeBool]\n",
    "df2016newTrueSID = df2016new[\"SERVICEID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Unfortunately, only some of the true label SERVICEIDs exist out of the now 10 avaiable. We extract only the SERVICEIDs that both exist in our 10 avaible and\n",
    "# in the Service Power Consumption logs\n",
    "\n",
    "file = pd.read_csv(\"Sub-Electric-2016-\" + str(1)+\".csv\")\n",
    "uniqueSID = list(file[\"SERVICEID\"].unique())\n",
    "\n",
    "uniqueSID = set(uniqueServiceID)\n",
    "df2016newTrueSID = set(df2016new[\"SERVICEID\"])\n",
    "availSID = uniqueSID & df2016newTrueSID\n",
    "availSID = list(availSID)\n",
    "\n",
    "df2016newprime = df2016new.loc[df2016new['SERVICEID'].isin(availSID)]\n",
    "\n",
    "# We do the same thing with our created dictionary (getting a subset)\n",
    "dict_avail =  {}\n",
    "dict_avail = {key: value for key, value in tempDict.items() for sublist in value if sublist in availSID}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Step 3 - (Creating features based on Postal Code)\n",
    "Now we need more features to increase the complexity of our dataset. Having more features gives us more options in evaluating more combination of features. \n",
    "<br> You may choose to drop features that have weak prediction outcome during the validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) In this step we try to get the power consumption (kwh) for service within the time frame of the faulted serviceID \n",
    "# (eg. if my meter was installed in 2016-07-20 and fault occured 2016-7-31 then I would get the power consumption for 11 days, and we do this for all SERVICEIDs\n",
    "# that have the same postal code as the one that has faulted)\n",
    "# Create an output file where you are able to get a file filled with kWh\n",
    "\n",
    "kWHprime = pd.DataFrame({\"SERVICEID\":[],\"START_YEAR\":[],\"START_MONTH\":[],\"START_DAY\":[],\"END_YEAR\":[],\"END_MONTH\":[],\"END_DAY\":[],\n",
    "                         \"DAY_DIFF\":[],\"KWH\":[], \"MAX_KWH\":[], \"PEAK_DAY_DIFF\":[], \"POST_CODE\":[]})\n",
    "\n",
    "## This would be every value in the subset of the POST_CODE\n",
    "for index, row in df2016newprime.iterrows():\n",
    "    \n",
    "    countSIDs = 0 \n",
    "    \n",
    "    print(row[\"POST_CODE\"])\n",
    "    print(\"SERVICEID:\",row[\"SERVICEID\"])\n",
    "    \n",
    "    sYear = int(row[\"START_YEAR\"])\n",
    "    sMonth = int(row[\"START_MONTH\"])\n",
    "    sDay = int(row[\"START_DAY\"])\n",
    "    \n",
    "    eYear = int(row[\"END_YEAR\"])\n",
    "    eMonth = int(row[\"END_MONTH\"])\n",
    "    eDay = int(row[\"END_DAY\"])\n",
    "    \n",
    "    totDaydiff = days_between(sYear, sMonth, sDay, eYear, eMonth, eDay)\n",
    "    print(totDaydiff)\n",
    "    \n",
    "    for value in dict_avail[row[\"POST_CODE\"]]:\n",
    "        print(\"ServiceID:\",value)\n",
    "        maxKWH = 0\n",
    "        midkWH = 0\n",
    "        #Reading the files\n",
    "        startfile = \"Sub-Electric-2016-\" + str(int(sMonth))\n",
    "        print(startfile)\n",
    "        startkWhTemp = pd.read_csv(startfile+\".csv\")\n",
    "        \n",
    "        \n",
    "        endfile = \"Sub-Electric-2016-\" + str(int(eMonth))\n",
    "        print(endfile)\n",
    "        endkWhTemp = pd.read_csv(endfile+\".csv\")\n",
    "        \n",
    "        \n",
    "        #isolating for specific service ids\n",
    "        startdfSID = startkWhTemp.loc[startkWhTemp[\"SERVICEID\"] == value]\n",
    "        enddfSID = endkWhTemp.loc[endkWhTemp[\"SERVICEID\"] == value]\n",
    "        \n",
    "        #summing total kwh for the start and end month\n",
    "        startdfSIDkWH = startdfSID.loc[startdfSID[\"DAYID\"]>=sDay].sum()[\"KWH\"]\n",
    "        enddfSIDkWH = enddfSID.loc[enddfSID[\"DAYID\"]<=eDay].sum()[\"KWH\"]\n",
    "        \n",
    "        #finding the max KWH for the start month\n",
    "        startdfmaxkWH = startdfSID.loc[startdfSID[\"DAYID\"]>=sDay].max()[\"KWH\"]\n",
    "        startdfdaykWH = startdfSID.loc[startdfSID[\"KWH\"] == startdfmaxkWH][\"DAYID\"].iloc[-1] \n",
    "        startdfmonthkWH = startdfSID.loc[startdfSID[\"KWH\"] == startdfmaxkWH][\"MONTHID\"].iloc[-1] \n",
    "        startdfyearkWH = startdfSID.loc[startdfSID[\"KWH\"] == startdfmaxkWH][\"YEARID\"].iloc[-1] \n",
    "        \n",
    "        #finding the max KWH for the end month\n",
    "        enddfmaxkWHtemp = enddfSID.loc[enddfSID[\"MONTHID\"]<=eMonth]\n",
    "        enddfmaxkWHtemp = enddfmaxkWHtemp.loc[enddfmaxkWHtemp[\"DAYID\"]<=eDay]\n",
    "        enddfmaxkWH = enddfmaxkWHtemp.max()[\"KWH\"]\n",
    "        \n",
    "        enddfdaykWH = enddfmaxkWHtemp.loc[enddfmaxkWHtemp[\"KWH\"] == enddfmaxkWH][\"DAYID\"].iloc[-1] \n",
    "        enddfmonthkWH = enddfmaxkWHtemp.loc[enddfmaxkWHtemp[\"KWH\"] == enddfmaxkWH][\"MONTHID\"].iloc[-1] \n",
    "        enddfyearkWH = enddfmaxkWHtemp.loc[enddfmaxkWHtemp[\"KWH\"] == enddfmaxkWH][\"YEARID\"].iloc[-1]\n",
    "\n",
    "        #finding the max KWH for the start month\n",
    "        if(startdfmaxkWH > enddfmaxkWH):\n",
    "            maxKWH = startdfmaxkWH\n",
    "            maxYr,maxMon,maxDay = startdfyearkWH,startdfmonthkWH,startdfdaykWH\n",
    "        else:\n",
    "            maxKWH = enddfmaxkWH\n",
    "            maxYr,maxMon,maxDay = enddfyearkWH,enddfmonthkWH,enddfdaykWH\n",
    "\n",
    "        #summing the total kWH for the middle monthes \n",
    "        for i in range(int(eMonth)-int(sMonth)-1):\n",
    "            midfile = \"Sub-Electric-2016-\" + str(int(sMonth)+i+1)\n",
    "            print(midfile)\n",
    "            midkWhTemp = pd.read_csv(midfile+\".csv\")\n",
    "            middfSIDkWHs = midkWhTemp.loc[midkWhTemp[\"SERVICEID\"] == value].sum()[\"KWH\"]\n",
    "            midkWH = midkWH + middfSIDkWHs\n",
    "            \n",
    "            middfmaxkWH = midkWhTemp.loc[midkWhTemp[\"SERVICEID\"] == value].max()[\"KWH\"]\n",
    "            middfdaykWH = midkWhTemp.loc[midkWhTemp[\"KWH\"] == middfmaxkWH][\"DAYID\"].iloc[-1] \n",
    "            middfmonthkWH = midkWhTemp.loc[midkWhTemp[\"KWH\"] == middfmaxkWH][\"MONTHID\"].iloc[-1] \n",
    "            middfyearkWH = midkWhTemp.loc[midkWhTemp[\"KWH\"] == middfmaxkWH][\"YEARID\"].iloc[-1] \n",
    "            \n",
    "            if (middfmaxkWH>maxKWH):\n",
    "                maxKWH = middfmaxkWH\n",
    "                maxYr,maxMon,maxDay = middfyearkWH,middfmonthkWH,middfdaykWH\n",
    "            else:\n",
    "                continue\n",
    "        countSIDs = countSIDs + 1\n",
    "        totalkWH = startdfSIDkWH + enddfSIDkWH + midkWH\n",
    "        PeakDaydiff = days_between(sYear, sMonth, sDay, maxYr, maxMon, maxDay)\n",
    "        totDaydiff = days_between(sYear, sMonth, sDay, eYear, eMonth, eDay)\n",
    "        kWHprime = kWHprime.append({\"SERVICEID\":value,\"START_YEAR\":sYear,\"START_MONTH\":sMonth,\"START_DAY\":sDay,\"START_YEAR\":sYear,\n",
    "                                    \"END_YEAR\":eYear,\"END_MONTH\":eMonth,\"END_DAY\":eDay, \"DAY_DIFF\": totDaydiff ,\"KWH\":totalkWH,\"MAX_KWH\":maxKWH, \n",
    "                                    \"PEAK_DAY_DIFF\":(totDaydiff-PeakDaydiff), \"POST_CODE\":row[\"POST_CODE\"]},ignore_index=True)\n",
    "kWHprime.to_csv(\"kWHprime.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 17)\n"
     ]
    }
   ],
   "source": [
    "# (6) **Note: This is purely based on the assumption that the POST code only failed once in the year. This will be incorrect otherwise\n",
    "\n",
    "#Get the number of service IDs in a postal Region\n",
    "kWHprimeNew = kWHprime\n",
    "kWHprimeUnqiue = kWHprime[\"POST_CODE\"].unique()\n",
    "templs = []\n",
    "for postcode in kWHprimeUnqiue:\n",
    "    templs.append(kWHprime[\"POST_CODE\"].str.count(postcode).sum())\n",
    "tempData = {\"POST_CODE\":kWHprimeUnqiue,\"POST_COUNT\":templs}\n",
    "tempdf = pd.DataFrame(tempData)\n",
    "\n",
    "#Get Average KWH for each service ID\n",
    "kWHprimeNew[\"AVG_KWH\"] = kWHprimeNew[\"KWH\"]/kWHprimeNew[\"DAY_DIFF\"]\n",
    "\n",
    "#Get Average KWH for each postal region\n",
    "kWHprimeAvgKWHPost = kWHprimeNew[[\"AVG_KWH\",\"POST_CODE\"]]\n",
    "kWHprimeAvgKWHPost = kWHprimeAvgKWHPost.groupby(['POST_CODE']).sum()\n",
    "kWHprimeAvgKWHPost.columns.values[0] = \"POST_AVG_KWH\" \n",
    "kWHprimeAvgKWHPost = pd.merge(tempdf, kWHprimeAvgKWHPost, left_on = 'POST_CODE', right_on = 'POST_CODE', how = 'left')\n",
    "kWHprimeAvgKWHPost[\"POST_AVG_KWH\"] = kWHprimeAvgKWHPost[\"POST_AVG_KWH\"]/kWHprimeAvgKWHPost[\"POST_COUNT\"]\n",
    "\n",
    "#Get Total KWH for each postal region\n",
    "kWHprimeTotKWHPost = kWHprimeAvgKWHPost\n",
    "kWHprimeTotKWHPost\n",
    "kWHprimeTotKWHPost = kWHprimeNew[[\"KWH\",\"POST_CODE\"]]\n",
    "kWHprimeTotKWHPost = kWHprimeTotKWHPost.groupby(['POST_CODE']).sum()\n",
    "kWHprimeTotKWHPost.columns.values[0] = \"POST_TOT_KWH\" \n",
    "kWHprimeTotKWHPost = pd.merge(kWHprimeAvgKWHPost, kWHprimeTotKWHPost, left_on = 'POST_CODE', right_on = 'POST_CODE', how = 'left')\n",
    "kWHprimeTotKWHPost[\"POST_TOT_KWH\"] = kWHprimeTotKWHPost[\"POST_TOT_KWH\"]\n",
    "\n",
    "#Get the Peak KWH for the region.\n",
    "templs = []\n",
    "for postcode in kWHprimeUnqiue:\n",
    "    templs.append(kWHprimeNew[kWHprimeNew['POST_CODE'].isin([postcode])][\"MAX_KWH\"].max())\n",
    "tempData = {\"POST_CODE\":kWHprimeUnqiue,\"POST_MAX_KWH\":templs}\n",
    "tempdf = pd.DataFrame(tempData)    \n",
    "kWHprimeNew = pd.merge(kWHprimeNew, tempdf, left_on = 'POST_CODE', right_on = 'POST_CODE', how = 'left')\n",
    "kWHprimeNew = pd.merge(kWHprimeNew, kWHprimeTotKWHPost, left_on = 'POST_CODE', right_on = 'POST_CODE', how = 'left')\n",
    "kWHprimeNew\n",
    "\n",
    "# Reorder and Rename KWH to TOT_KWH\n",
    "kWHprimeNew.columns.values[8] = \"TOT_KWH\" \n",
    "kWHprimeNew = kWHprimeNew[kWHprimeNew[\"SERVICEID\"].isin(availSID)]\n",
    "print(kWHprimeNew.shape) #There is only 2 true values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models Step 4 - Training and Testing the Dataset \n",
    "### TO BE CONITNUED BY QMIND 2020-2021\n",
    "Down below I provided a pipeline and necessary libraries needed to run the machine learning classifier. \n",
    "<br> When you are training the model, I would ensure that you drop the DAYS_DIFF features. Based on intution I think this will lead to overfitting.\n",
    "<br> Also when getting negative (meters that haven't faulted) I would use the default DAYS_DIFF as 1000 and capture the last 1000 days. This is just so you can fill in than putting a null-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports necessary to run the pipeline function I provided\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from statistics import mean\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification pipeline that has 5-fold stratified cross validation\n",
    "def model(df,classifer,title):\n",
    "#Splitting up X and Y\n",
    "    print(df)\n",
    "    X = df.iloc[:,0:-1]\n",
    "    X = X.values\n",
    "    y = df.iloc[:,-1:]\n",
    "    y = y.values\n",
    "\n",
    "    listPrecision = []\n",
    "    listRecall = []\n",
    "    listAuc = []\n",
    "    listF1 = []\n",
    "    listCM = []\n",
    "    listMCC = []\n",
    "\n",
    "    countFold = 1\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, random_state = 351)\n",
    "    for train_idx, test_idx, in cv.split(X,y):    \n",
    "\n",
    "    print(\"Running fold:\", countFold)\n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:] \n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "\n",
    "    clf = classifer\n",
    "\n",
    "    model = Pipeline([('RF', clf)])\n",
    "    model.set_params(RF__random_state=351).fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    precision = precision_score(y_test, y_pred) #scores for random forest model\n",
    "    listPrecision.append(round(precision,5))\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    listRecall.append(round(recall,5))\n",
    "    auc = roc_auc_score(y_test,y_pred)\n",
    "    listAuc.append(round(auc,5))\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    listF1.append(round(f1,5))\n",
    "    mcc = matthews_corrcoef(y_test,y_pred)\n",
    "    listMCC.append(round(mcc,5))\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_test,y_pred)   \n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print('Classification report \\n {} \\n'.format(classification_report(y_test, y_pred)))\n",
    "    print(\"Confusion Matrix: \", cm)\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    sns.set(font_scale=2) \n",
    "    sns.heatmap(cmn, annot=True, ax=ax, cmap=\"Blues\", fmt=\".2g\");  \n",
    "\n",
    "\n",
    "    label_font = {'size':'20'}  \n",
    "    ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "    ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16) \n",
    "    ax.xaxis.set_ticklabels(['Non-PD', 'PD']);\n",
    "    ax.yaxis.set_ticklabels(['Non-PD', 'PD']);\n",
    "    plt.show()\n",
    "    countFold = countFold+1\n",
    "\n",
    "\n",
    "\n",
    "    print(title + \" Precision Score: \",listPrecision , \" |Average Precision Score: \", mean(listPrecision))\n",
    "    print(title + \" Recall Score: \",listRecall,  \" |Average Recall Score: \", mean(listRecall))\n",
    "    print(title + \" AUC Score: \",listAuc,  \" |Average AUC Score: \", mean(listAuc))\n",
    "    print(title + \" F1 Score: \",listF1,  \" |Average F1 Score: \", mean(listF1))\n",
    "    print(title + \" MCC Score: \",listMCC,  \" |Average MCC Score: \", mean(listMCC))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifer call\n",
    "clfBL = model(df,RandomForestClassifier(random_state=351),\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "I have provided the necessary files to be able to give you an example of out some of the dataframes should look.\n",
    "The \"KWH_LESS.csv\", \"KWH_MAX.csv\", and \"KWH_Zeros.csv\" maybe be useful to look at for future analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
